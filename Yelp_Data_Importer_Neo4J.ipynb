{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Preparing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary libraries.\n",
    "import os, subprocess, uuid\n",
    "import regex as re\n",
    "import datetime\n",
    "from time import sleep\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json, csv\n",
    "from typing import List, Tuple, Set\n",
    "from pathlib import Path\n",
    "\n",
    "from py2neo import Graph\n",
    "from neo4j.exceptions import ServiceUnavailable, TransientError\n",
    "\n",
    "import shutil\n",
    "import reverse_geocoder as rg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Const.\n",
    "IN_CATEGORY = \"IN_CATEGORY\"\n",
    "IN_CITY = \"IN_CITY\"\n",
    "IN_AREA = \"IN_AREA\"\n",
    "IN_COUNTRY = \"IN_COUNTRY\"\n",
    "FRIENDS = \"FRIENDS\"\n",
    "REVIEWS = \"REVIEWS\"\n",
    "WROTE = \"WROTE\"\n",
    "BUSINESS_NODE = \"Business\"\n",
    "USER_NODE = \"User\"\n",
    "REVIEW_NODE = \"Review\"\n",
    "CATEGORY_NODE = \"Category\"\n",
    "CITY_NODE = \"City\"\n",
    "AREA_NODE = \"Area\"\n",
    "COUNTRY_NODE = \"Country\"\n",
    "\n",
    "CATEGORY_FILTER = [\"Restaurants\", \"Food\", \"Fast Food\", \"Sandwiches\", \"Breakfast & Brunch\", \n",
    "                   \"Coffee & Tea\", \"American\", \"Burgers\", \"Mexican\", \"Desserts\", \n",
    "                   \"Specialty Food\", \"Pizza\", \"Salad\", \"Italian\", \"Chicken Wings\",\n",
    "                  \"Tacos\", \"Seafood\", \"Vegetarian\", \"Ethnic Food\", \"Chinese\", \n",
    "                   \"Barbeque\", \"Mediterranean\", \"Japanese\", \"Korean\", \"Indian\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important locations.\n",
    "data_folder = r\"../../Dataset/yelp\"\n",
    "business_json_file = data_folder + \"/json/yelp_academic_dataset_business.json\"\n",
    "review_json_file = data_folder + \"/json/yelp_academic_dataset_review.json\"\n",
    "user_json_file = data_folder + \"/json/yelp_academic_dataset_user.json\"\n",
    "checkin_json_file = data_folder + \"/json/yelp_academic_dataset_checkin.json\"\n",
    "photos_json_file = data_folder + \"/photos/photos.json\"\n",
    "fixed_business_json_file = data_folder + \"/json/fixed_yelp_academic_dataset_business.json\"\n",
    "fixed_review_json_file = data_folder + \"/json/fixed_yelp_academic_dataset_review.json\"\n",
    "fixed_user_json_file = data_folder + \"/json/fixed_yelp_academic_dataset_user.json\"\n",
    "fixed_checkin_json_file = data_folder + \"/json/fixed_yelp_academic_dataset_checkin.json\"\n",
    "fixed_photos_json_file = data_folder + \"/json/fixed_photos.json\"\n",
    "list_raw_files = [business_json_file, review_json_file, user_json_file, checkin_json_file, photos_json_file]\n",
    "list_fixed_data = [fixed_business_json_file, fixed_review_json_file, fixed_user_json_file, \n",
    "                   fixed_checkin_json_file, fixed_photos_json_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Csv files for Import Tool.\n",
    "# business_nodes_csv_file = data_folder + \"/neo4j/csv/business_nodes.csv\"\n",
    "# category_nodes_csv_file = data_folder + \"/neo4j/csv/category_nodes.csv\"\n",
    "# city_nodes_csv_file = data_folder + \"/neo4j/csv/city_nodes.csv\"\n",
    "# area_nodes_csv_file = data_folder + \"/neo4j/csv/area_nodes.csv\"\n",
    "# country_nodes_csv_file = data_folder + \"/neo4j/csv/country_nodes.csv\"\n",
    "# user_nodes_csv_file = data_folder + \"/neo4j/csv/user_nodes.csv\"\n",
    "# review_nodes_csv_file = data_folder + \"/neo4j/csv/review_nodes.csv\"\n",
    "# relationship_csv_file = data_folder + \"/neo4j/csv/relationships.csv\"\n",
    "# nodes_files = [business_nodes_csv_file, category_nodes_csv_file, city_nodes_csv_file, \n",
    "#                area_nodes_csv_file, country_nodes_csv_file, user_nodes_csv_file, review_nodes_csv_file]\n",
    "\n",
    "business_nodes_csv_file = data_folder + \"/neo4j/csv/restaurant_nodes.csv\"\n",
    "category_nodes_csv_file = data_folder + \"/neo4j/csv/category_nodes.csv\"\n",
    "checkin_nodes_csv_file = data_folder + \"/neo4j/csv/checkin_nodes.csv\"\n",
    "hour_nodes_csv_file = data_folder + \"/neo4j/csv/hour_nodes.csv\"\n",
    "menu_nodes_csv_file = data_folder + \"/neo4j/csv/menu_nodes.csv\"\n",
    "photo_nodes_csv_file = data_folder + \"/neo4j/csv/photo_nodes.csv\"\n",
    "city_nodes_csv_file = data_folder + \"/neo4j/csv/city_nodes.csv\"\n",
    "area_nodes_csv_file = data_folder + \"/neo4j/csv/state_nodes.csv\"\n",
    "country_nodes_csv_file = data_folder + \"/neo4j/csv/country_nodes.csv\"\n",
    "user_nodes_csv_file = data_folder + \"/neo4j/csv/user_nodes.csv\"\n",
    "review_nodes_csv_file = data_folder + \"/neo4j/csv/review_nodes.csv\"\n",
    "relationship_csv_file = data_folder + \"/neo4j/csv/relationships.csv\"\n",
    "nodes_files = [business_nodes_csv_file, category_nodes_csv_file, checkin_nodes_csv_file,\n",
    "               hour_nodes_csv_file, menu_nodes_csv_file, photo_nodes_csv_file, city_nodes_csv_file, \n",
    "               area_nodes_csv_file, country_nodes_csv_file, user_nodes_csv_file, review_nodes_csv_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection to Neo4j Db.\n",
    "graph_name = \"neo4j\" # Default for 4.0: neo4j; default for 3.5: graph.db\n",
    "SERVER_ADDRESS = \"bolt://localhost:7687\"\n",
    "SERVER_AUTH = (\"neo4j\", \"erclab\")\n",
    "graph = Graph(SERVER_ADDRESS, auth=SERVER_AUTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_home = graph.service.config[\"dbms.directories.neo4j_home\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/muzamil/.config/Neo4j Desktop/Application/neo4jDatabases/database-5aa8fa9e-f0fd-49c2-938f-cfec39577c04/installation-4.1.0\n"
     ]
    }
   ],
   "source": [
    "print(neo4j_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not ok\n"
     ]
    }
   ],
   "source": [
    "# Check neo4j connection\n",
    "try:\n",
    "    graph.run(\"Match () Return 1 Limit 1\")\n",
    "    print('ok')\n",
    "except Exception:\n",
    "    print('not ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_files(files: List[str]):\n",
    "    for one_file in files:\n",
    "        one_path = Path(one_file)\n",
    "        if one_path.is_file():\n",
    "            one_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not all(list(map(lambda x: Path(x).is_file(), list_raw_files))):\n",
    "    raise Exception(f'Not all Yelp raw files are available. Need: {list_raw_files}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Preprocessing Data</h1>\n",
    "\n",
    "<b>Data Problems:</b>\n",
    "\n",
    "    1. Ids are unique only in the specific domain. E.g. business_id in Business oiAlXZPIFm2nBCt0DHLu_Q is identical to that user_id in User. When using Neo4j's Import Tool, the relationship csv defines \"from-node\" and \"to-node\", identified by unique ids. Thus, the entities' ids, regardless of Business or User or Review must be unique.\n",
    "    2. Users have friends, but not all the friends exist. E.g. Unknown friend: oeMvJh94PiGQnx_6GlndPQ for User ntlvfPzc8eglqvk92iDIAw. Those unknown friends must be removed.\n",
    "    3. Some Business contain duplicate categories, e.g. Business with id HEGy1__jKyMhkhXRW3O1ZQ has duplicated Gas Stations. These must be deduplicated.\n",
    "    4. In Users, friends is supposed to be an array with string elements. But in reality, it is a string in which friend_ids are concatenated with commas. Same problem with categories field in Business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix data problems.\n",
    "def remove_unknown_friends(raw_path: str, output_path: str) -> None:\n",
    "    user_ids = set()\n",
    "    with open(raw_path, \"r\", encoding=\"utf-8\") as rf:\n",
    "        for line in rf:\n",
    "            if len(line.strip()) > 0:\n",
    "                json_node = json.loads(line)\n",
    "                user_ids.add(json_node[\"user_id\"])\n",
    "            \n",
    "    with open(raw_path, \"r\", encoding=\"utf-8\") as rf, open(output_path, mode=\"w\", encoding=\"utf-8\") as of:\n",
    "        for line in rf:\n",
    "            if len(line.strip()) > 0:\n",
    "                json_node = json.loads(line)\n",
    "                friends_str = json_node[\"friends\"]\n",
    "                if friends_str and len(friends_str.strip()) > 0:\n",
    "                    friends_arr = re.split(r\"\\s*,\\s*\", friends_str.strip())\n",
    "                    friends_exist_arr = list(filter(lambda x: x in user_ids, friends_arr))\n",
    "                    json_node[\"friends\"] = ', '.join(friends_exist_arr)\n",
    "                    of.write(f'{json.dumps(json_node)}\\n')\n",
    "\n",
    "                    \n",
    "def make_ids_unique(input_path: str, output_path: str, func_to_modify) -> None:\n",
    "    temp_output_file = str(uuid.uuid4())\n",
    "    line_count = len(open(input_path).readlines())\n",
    "    with open(input_path, mode=\"r\", encoding=\"utf-8\") as in_f, open(temp_output_file, mode=\"w\", encoding=\"utf-8\") as ou_f:\n",
    "        for line in tqdm(in_f, total=line_count):\n",
    "            if len(line.strip())>0:\n",
    "                json_node = json.loads(line)\n",
    "                json_node = func_to_modify(json_node)\n",
    "                if json_node is not None :\n",
    "                    ou_f.write(f'{json.dumps(json_node)}\\n')\n",
    "    # Rename the file.\n",
    "    os.replace(temp_output_file, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_food_businesses():\n",
    "    business_ids = []\n",
    "    line_count = len(open(fixed_business_json_file).readlines())\n",
    "    with open(fixed_business_json_file) as ref:\n",
    "        for line in tqdm(ref, total=line_count):\n",
    "            blob = json.loads(line)\n",
    "            business_ids += [blob[\"business_id\"]]\n",
    "        print(str(len(business_ids))+\" \"+business_ids[0])\n",
    "    return business_ids\n",
    "\n",
    "business_ids = get_food_businesses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_foodie_reviewer():\n",
    "    user_ids = []\n",
    "    line_count = len(open(fixed_review_json_file).readlines())\n",
    "    with open(fixed_review_json_file) as ref:\n",
    "        for line in tqdm(ref, total=line_count):\n",
    "            blob = json.loads(line)\n",
    "            user_ids += [blob[\"user_id\"]]\n",
    "        print(str(len(user_ids))+\" \"+user_ids[0])\n",
    "    return user_ids\n",
    "\n",
    "user_ids = get_foodie_reviewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_user(json_node):\n",
    "        json_node[\"user_id\"] = \"u-\" + json_node[\"user_id\"]\n",
    "        friends_str = json_node[\"friends\"]\n",
    "        if friends_str and len(friends_str.strip()) > 0:\n",
    "            friends_arr = re.split(r\"\\s*,\\s*\", friends_str.strip())\n",
    "            friends_arr = list(map(lambda x: \"u-\" + x, friends_arr))\n",
    "            json_node[\"friends\"] = ', '.join(friends_arr)\n",
    "        return json_node\n",
    "#         if df[0].eq(json_node[\"user_id\"]).any():\n",
    "#             return json_node\n",
    "#         else :\n",
    "#             return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1968703/1968703 [00:58<00:00, 33936.85it/s]\n"
     ]
    }
   ],
   "source": [
    "make_ids_unique(fixed_user_json_file, fixed_user_json_file, fix_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_users_with_review(input_path: str, output_path: str) -> None:\n",
    "    temp_output_file = str(uuid.uuid4())\n",
    "    line_count = len(open(input_path).readlines())\n",
    "    with open(input_path, mode=\"r\", encoding=\"utf-8\") as in_f, open(temp_output_file, mode=\"w\", encoding=\"utf-8\") as ou_f:\n",
    "        for line in tqdm(in_f, total=line_count):\n",
    "            if len(line.strip())>0:\n",
    "                json_node = json.loads(line)\n",
    "                json_df = pd.DataFrame([json_node])\n",
    "                if df[0].eq(json_df[\"user_id\"]).any():\n",
    "                    ou_f.write(f'{json.dumps(json_node)}\\n')\n",
    "    # Rename the file.\n",
    "    os.replace(temp_output_file, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_users_with_review(fixed_user_json_file, fixed_user_json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the fixed files are already available.\n",
    "if not all(list(map(lambda x: Path(x).is_file(), list_fixed_data))):\n",
    "    # Remove everything first.\n",
    "    delete_files(list_fixed_data)\n",
    "    # Re-create the fixed files.\n",
    "    def fix_user(json_node):\n",
    "        json_node[\"user_id\"] = \"u-\" + json_node[\"user_id\"]\n",
    "        friends_str = json_node[\"friends\"]\n",
    "        if friends_str and len(friends_str.strip()) > 0:\n",
    "            friends_arr = re.split(r\"\\s*,\\s*\", friends_str.strip())\n",
    "            friends_arr = list(map(lambda x: \"u-\" + x, friends_arr))\n",
    "            json_node[\"friends\"] = ', '.join(friends_arr)\n",
    "        return json_node\n",
    "        \n",
    "    def fix_business(json_node):\n",
    "        json_node[\"business_id\"] = \"b-\" + json_node[\"business_id\"]\n",
    "        if json_node[\"categories\"] and len(json_node[\"categories\"].strip())>0:\n",
    "            categories_arr = re.split(\"\\s*,\\s*\", json_node[\"categories\"].strip())\n",
    "            categories_set = set(categories_arr)\n",
    "            category_food_filter = list(filter(lambda x: x in CATEGORY_FILTER, categories_set))\n",
    "            if len(category_food_filter) > 0 :\n",
    "                json_node[\"categories\"] = ', '.join(categories_set)\n",
    "                return json_node\n",
    "            else :\n",
    "                return None\n",
    "            \n",
    "    def fix_review(json_node):\n",
    "        json_node[\"review_id\"] = \"r-\" + json_node[\"review_id\"]\n",
    "        json_node[\"user_id\"] = \"u-\" + json_node[\"user_id\"]\n",
    "        json_node[\"business_id\"] = \"b-\" + json_node[\"business_id\"]\n",
    "        if any(json_node[\"business_id\"] in s for s in business_ids):\n",
    "            return json_node\n",
    "        else :\n",
    "            return None\n",
    "        \n",
    "    def fix_checkin(json_node):\n",
    "        json_node[\"business_id\"] = \"b-\" + json_node[\"business_id\"]\n",
    "        if any(json_node[\"business_id\"] in s for s in business_ids):\n",
    "            return json_node\n",
    "        else :\n",
    "            return None\n",
    "        \n",
    "    def fix_photos(json_node):\n",
    "        json_node[\"business_id\"] = \"b-\" + json_node[\"business_id\"]\n",
    "        if any(json_node[\"business_id\"] in s for s in business_ids):\n",
    "            return json_node\n",
    "        else :\n",
    "            return None\n",
    "\n",
    "    # Start fixing data problems.\n",
    "    remove_unknown_friends(user_json_file, fixed_user_json_file)\n",
    "    make_ids_unique(fixed_user_json_file, fixed_user_json_file, fix_user)\n",
    "    make_ids_unique(business_json_file, fixed_business_json_file, fix_business)\n",
    "    make_ids_unique(review_json_file, fixed_review_json_file, fix_review)\n",
    "    make_ids_unique(checkin_json_file, fixed_checkin_json_file, fix_checkin)\n",
    "    make_ids_unique(photos_json_file, fixed_photos_json_file, fix_photos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Importing Data</h1>\n",
    "\n",
    "<h3>Generating inputs for Import Tool</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not all(list(map(lambda x: Path(x).is_file(), nodes_files))) or not Path(relationship_csv_file).is_file():\n",
    "    # Delete everything.\n",
    "    delete_files(nodes_files)\n",
    "    delete_files([relationship_csv_file])\n",
    "    # Using sets to ensures uniqueness.\n",
    "    business_lat_lon = {}\n",
    "    area_nodes: Set[Tuple[str, str]] = set()\n",
    "    city_nodes: Set[Tuple[str, str]]= set()\n",
    "    country_nodes: Set[str] = set()\n",
    "    categories_nodes: Set[str] = set()\n",
    "    in_area_relationships: Set[Tuple[str, str, str]] = set()\n",
    "    in_country_relationships: Set[Tuple[str, str, str]] = set()\n",
    "    # Relationship writer.\n",
    "    relationship_csv = open(relationship_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\\n\")\n",
    "    relationship_fieldnames = [\":START_ID\", \":END_ID\", \":TYPE\"]\n",
    "    relationship_writer = csv.DictWriter(relationship_csv, fieldnames=relationship_fieldnames, \n",
    "                                         delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    relationship_writer.writeheader()\n",
    "    \n",
    "    # Func to write Business Nodes to file.\n",
    "    def write_business_nodes_to_file():\n",
    "        with open(fixed_business_json_file, \"r\", encoding=\"utf-8\") as bjf, open(\n",
    "            business_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\\n\") as business_nodes_csv:\n",
    "            fieldnames = [\"business_id:ID\", \"name\", \"address\", \":LABEL\"]\n",
    "            writer = csv.DictWriter(business_nodes_csv, fieldnames=fieldnames, delimiter=\",\", \n",
    "                                    quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writeheader()\n",
    "            # File too large, read line by line.\n",
    "            for line in bjf:\n",
    "                line = line.strip()\n",
    "                if len(line)>0:\n",
    "                    json_node = json.loads(line)\n",
    "                    writer.writerow({k:v for k, v in zip(fieldnames, [json_node[\"business_id\"], \n",
    "                                                                      json_node[\"name\"], json_node[\"address\"], \n",
    "                                                                      BUSINESS_NODE])})\n",
    "                    business_lat_lon[json_node[\"business_id\"]] = (json_node[\"latitude\"], json_node[\"longitude\"])\n",
    "                    if json_node[\"categories\"] and len(json_node[\"categories\"].strip())>0: \n",
    "                        # can be None (e.g. for Business Id: 2W1tLg8ybRUEKMPoAPHTsQ)\n",
    "                        cur_categories = list(filter(lambda x: len(x)>0, \n",
    "                                                     map(lambda x: x.strip(), \n",
    "                                                         re.split(\"\\s*,\\s*\", json_node[\"categories\"].strip()))))\n",
    "                        categories_nodes.update(cur_categories)\n",
    "                        for category in cur_categories:\n",
    "                            relationship_writer.writerow({k:v for k, v in zip(relationship_fieldnames, \n",
    "                                                                              [json_node[\"business_id\"], \n",
    "                                                                               category, IN_CATEGORY])})\n",
    "    \n",
    "    # Func to write Category Nodes to file.\n",
    "    def write_category_nodes_to_file():\n",
    "        with open(category_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\\n\") as category_nodes_csv:\n",
    "            fieldnames = [\"category_id:ID\", \":LABEL\"]\n",
    "            writer = csv.DictWriter(category_nodes_csv, fieldnames=fieldnames, delimiter=\",\", \n",
    "                                    quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writeheader()\n",
    "            for category_id in categories_nodes:\n",
    "                writer.writerow({k:v for k, v in zip(fieldnames, [category_id, CATEGORY_NODE])})\n",
    "                \n",
    "    # Func to make City, Area, Country Nodes.\n",
    "    def make_city_area_country_nodes():\n",
    "        lat_lons = list(business_lat_lon.values())\n",
    "        city_state_countries = rg.search(lat_lons)\n",
    "        # Process City, Area, Country Nodes; save IN_CITY relationship to file.\n",
    "        for (business_id, city_state_country) in list(zip(list(business_lat_lon.keys()), city_state_countries)):\n",
    "            city, state, country = city_state_country[\"name\"], city_state_country[\"admin1\"], city_state_country[\"cc\"]\n",
    "            unique_state = f'{state}-{country}'\n",
    "            unique_city = f'{city}-{state}-{country}'\n",
    "            country_nodes.add(country)\n",
    "            area_nodes.add((unique_state, state))\n",
    "            city_nodes.add((unique_city, city))\n",
    "            # Create corresponding relationships.\n",
    "            relationship_writer.writerow({k:v for k, v in zip(relationship_fieldnames, \n",
    "                                                              [business_id, unique_city, IN_CITY])})\n",
    "            in_area_relationships.add((unique_city, unique_state, IN_AREA))\n",
    "            in_country_relationships.add((unique_state, country, IN_COUNTRY))\n",
    "    \n",
    "    # Func to write City, Area, Country Nodes to file.\n",
    "    def write_city_area_country_nodes_to_file():\n",
    "        with open(city_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\\n\") as city_nodes_csv:\n",
    "            fieldnames = [\"city_id:ID\", \"name\", \":LABEL\"]\n",
    "            writer = csv.DictWriter(city_nodes_csv, fieldnames=fieldnames, delimiter=\",\", \n",
    "                                    quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writeheader()\n",
    "            for (city_id, city_name) in city_nodes:\n",
    "                writer.writerow({k:v for k, v in zip(fieldnames, [city_id, city_name, CITY_NODE])})\n",
    "\n",
    "        with open(area_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\\n\") as area_nodes_csv:\n",
    "            fieldnames = [\"area_id:ID\", \"name\", \":LABEL\"]\n",
    "            writer = csv.DictWriter(area_nodes_csv, fieldnames=fieldnames, delimiter=\",\", \n",
    "                                    quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writeheader()\n",
    "            for (area_id, area_name) in area_nodes:\n",
    "                writer.writerow({k:v for k, v in zip(fieldnames, [area_id, area_name, AREA_NODE])})\n",
    "\n",
    "        with open(country_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\\n\") as country_nodes_csv:\n",
    "            fieldnames = [\"country_id:ID\", \":LABEL\"]\n",
    "            writer = csv.DictWriter(country_nodes_csv, fieldnames=fieldnames, delimiter=\",\", \n",
    "                                    quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writeheader()\n",
    "            for country_id in country_nodes:\n",
    "                writer.writerow({k:v for k, v in zip(fieldnames, [country_id, COUNTRY_NODE])})\n",
    "                \n",
    "        for (u1, u2, rel_type) in in_area_relationships:\n",
    "            relationship_writer.writerow({k:v for k, v in zip(relationship_fieldnames, [u1, u2, rel_type])})\n",
    "\n",
    "        for (u1, u2, rel_type) in in_country_relationships:\n",
    "            relationship_writer.writerow({k:v for k, v in zip(relationship_fieldnames, [u1, u2, rel_type])})\n",
    "    \n",
    "    # Func to write User nodes to file.\n",
    "    def write_user_nodes_to_file():\n",
    "        friend_relationships: Set[Tuple[str, str, str]] = set()\n",
    "        with open(fixed_user_json_file, \"r\", encoding=\"utf-8\") as ujf, open(user_nodes_csv_file, \n",
    "                                                                            mode=\"w\", encoding=\"utf-8\", \n",
    "                                                                            newline=\"\\n\") as user_nodes_csv:\n",
    "            fieldnames = [\"user_id:ID\", \"name\", \"yelping_since\", \":LABEL\"]\n",
    "            writer = csv.DictWriter(user_nodes_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', \n",
    "                                    quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writeheader()\n",
    "            # File too large, read line by line.\n",
    "            for line in ujf:\n",
    "                line = line.strip()\n",
    "                if len(line)>0:\n",
    "                    json_node = json.loads(line)\n",
    "                    writer.writerow({k:v for k, v in zip(fieldnames, [json_node[\"user_id\"], \n",
    "                                                                      json_node[\"name\"], \n",
    "                                                                      json_node[\"yelping_since\"], \n",
    "                                                                      USER_NODE])})\n",
    "                    if json_node[\"friends\"] and len(json_node[\"friends\"].strip()) > 0:\n",
    "                        friends_arr = re.split(\"\\s*,\\s*\", json_node[\"friends\"].strip())\n",
    "                        for friend_id in friends_arr:\n",
    "                            # Prevent duplicate friend relationship later!\n",
    "                            f1 = min(json_node[\"user_id\"], friend_id)\n",
    "                            f2 = max(json_node[\"user_id\"], friend_id)\n",
    "                            friend_relationships.add((f1, f2, FRIENDS))\n",
    "        \n",
    "        for (f1, f2, rel_type) in friend_relationships:\n",
    "            relationship_writer.writerow({k:v for k, v in zip(relationship_fieldnames, [f1, f2, rel_type])})\n",
    "    \n",
    "    # Func to write Review nodes to file.\n",
    "    def write_review_nodes_to_file():\n",
    "        with open(fixed_review_json_file, \"r\", encoding=\"utf-8\") as rjf, open(review_nodes_csv_file, \n",
    "                                                                              mode=\"w\", encoding=\"utf-8\", \n",
    "                                                                              newline=\"\\n\") as review_nodes_csv:\n",
    "            fieldnames = [\"review_id:ID\", \"stars\", \"date\", \"text\", \":LABEL\"]\n",
    "            writer = csv.DictWriter(review_nodes_csv, fieldnames=fieldnames, delimiter=\",\", \n",
    "                                    quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writeheader()\n",
    "            # File too large, read line by line.\n",
    "            for line in rjf:\n",
    "                line = line.strip()\n",
    "                if len(line)>0:\n",
    "                    json_node = json.loads(line)\n",
    "                    writer.writerow({k:v for k, v in zip(fieldnames, [json_node[\"review_id\"], \n",
    "                                                                      json_node[\"stars\"], json_node[\"date\"], \n",
    "                                                                      json_node[\"text\"], REVIEW_NODE])})\n",
    "                    relationship_writer.writerow({k:v for k, v in zip(relationship_fieldnames, \n",
    "                                                                      [json_node[\"user_id\"], \n",
    "                                                                       json_node[\"review_id\"], WROTE])})\n",
    "                    relationship_writer.writerow({k:v for k, v in zip(relationship_fieldnames, \n",
    "                                                                      [json_node[\"review_id\"], \n",
    "                                                                       json_node[\"business_id\"], REVIEWS])})\n",
    "    \n",
    "    # Start re-creating CSV files for Import Tool.\n",
    "    write_business_nodes_to_file()\n",
    "    write_category_nodes_to_file()\n",
    "    del categories_nodes\n",
    "    make_city_area_country_nodes()\n",
    "    write_city_area_country_nodes_to_file()\n",
    "    del business_lat_lon, city_nodes, area_nodes, country_nodes, in_area_relationships, in_country_relationships\n",
    "    write_user_nodes_to_file()\n",
    "    write_review_nodes_to_file()\n",
    "    \n",
    "    relationship_csv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Checking Data Integrity</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_nodes = 0\n",
    "\n",
    "# Func to if nodes are unique and relationships are also unique.\n",
    "def check_nodes_relationships_csv_files_integrity():\n",
    "    global num_of_nodes\n",
    "    # Nodes files.\n",
    "    for one_node_file in nodes_files:\n",
    "        temp_df = pd.read_csv(one_node_file, header = \"infer\", sep = \",\", encoding = \"utf-8\")\n",
    "        if len(temp_df.iloc[:, 0]) != len(np.unique(temp_df.iloc[:, 0].values)):\n",
    "            raise Exception(f'Nodes in [{one_node_file}]: not unique')\n",
    "        num_of_nodes += len(temp_df.iloc[:, 0])\n",
    "    # Relationship file.\n",
    "    temp_df = pd.read_csv(relationship_csv_file, header = \"infer\", sep = \",\", encoding = \"utf-8\")\n",
    "    rels = temp_df.iloc[:, 0] + temp_df.iloc[:, 1]\n",
    "    if len(rels) != len(np.unique(rels.values)):\n",
    "        raise Exception(f'Relationships in [{relationship_csv_file}]: not unique')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_nodes_relationships_csv_files_integrity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importing to Neo4j</h3>\n",
    "\n",
    "<b>Reseting the database and importing data</b>\n",
    "\n",
    "The quickest/cleanest/safest way is:\n",
    "\n",
    "    1. Stop the Neo4j's Database Service.\n",
    "    2. Remove the neo4j's Database's Data folder.\n",
    "    3. Execute the Import Tool.\n",
    "    4. Start the Neo4j's Database Service.\n",
    "\n",
    "<b>Important Note:</b> Users must have privileges to start/stop Windows services. If needing elevating, a Windows popup will appear, asking for the permission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Func to control Neo4j's Database Service.\n",
    "def command_neo4j_database_service(cmd: str):\n",
    "    neo4j_cmd = neo4j_home + r\"/bin/neo4j\"\n",
    "    if cmd in [\"stop\", \"start\"]:\n",
    "        cmd_res = subprocess.run([neo4j_cmd, cmd], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        cmd_stdout = str(cmd_res.stdout, \"utf-8\")\n",
    "        print(cmd_stdout)\n",
    "#         if cmd == \"stop\" and \"stopped\" not in cmd_stdout:\n",
    "#             raise Exception(f\"Can't stop Neo4j's Database Service [{cmd_res.stderr}]\")\n",
    "#         elif cmd == \"start\" and \"started\" not in cmd_stdout:\n",
    "#             raise Exception(f\"Can't start Neo4j's Database Service [{cmd_res.stderr}]\")\n",
    "    else:\n",
    "        raise Exception(f'Unknown command for Neo4j\\'s Database Service: [{cmd}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Func to remove the Database's Data folder.\n",
    "def reset_neo4j_database():\n",
    "    neo4j_graph_folder = rf\"{neo4j_home}/data/databases/{graph_name}\"\n",
    "    neo4j_trans_folder = rf\"{neo4j_home}/data/transactions/{graph_name}\"\n",
    "    print(neo4j_graph_folder)\n",
    "    print(neo4j_trans_folder)\n",
    "    if not Path(neo4j_graph_folder).is_dir():\n",
    "        raise Exception(\"Can't find Neo4j's Database's Data folder\")\n",
    "    shutil.rmtree(neo4j_graph_folder)\n",
    "    shutil.rmtree(neo4j_trans_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Func to import data into Neo4j's Database.\n",
    "def import_data():\n",
    "    import_tool_cmd = neo4j_home + r\"/bin/neo4j-admin\"\n",
    "    arguments = [\"import\", \"--multiline-fields=true\"] + list(map(lambda x: f'--nodes={x}', nodes_files)) + [f\"--relationships={relationship_csv_file}\"]\n",
    "    print(arguments)\n",
    "    # Execute the Import Tool (This tool will re-create a fresh \"neo4j\" folder if needed).\n",
    "    cmd_res = subprocess.run([import_tool_cmd] + arguments, cwd=os.getcwd(), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    print(str(cmd_res.stdout, \"utf-8\"))\n",
    "    if \"IMPORT DONE\" not in str(cmd_res.stdout, \"utf-8\"):\n",
    "        raise Exception(f\"Can't execute Import Tool successfully [{cmd_res.stderr}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Neo4j's Database Service is already containing the desired data.\n",
    "if not(len(graph.nodes) == num_of_nodes and num_of_nodes > 0):\n",
    "    # Start resetting the database, then importing the data (nodes & relationships).\n",
    "    command_neo4j_database_service(\"stop\")\n",
    "    reset_neo4j_database()\n",
    "    import_data()\n",
    "    command_neo4j_database_service(\"start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_graph_ready = False\n",
    "\n",
    "# Func to check if the importing process was done successfully.\n",
    "def check_if_importing_is_successful():\n",
    "    global graph\n",
    "    global yelp_graph_ready\n",
    "    num_tries = 30\n",
    "    for one_try in range(num_tries):\n",
    "        try:\n",
    "            graph = Graph(SERVER_ADDRESS, auth=SERVER_AUTH)\n",
    "            cur_num_nodes = len(graph.nodes)\n",
    "            if cur_num_nodes == 0:\n",
    "                raise Exception(f\"There is no node in the Database\")\n",
    "            if cur_num_nodes != num_of_nodes:\n",
    "                raise Exception(f'Expected: [{num_of_nodes}] nodes, but found: [{cur_num_nodes}]')\n",
    "            yelp_graph_ready = True\n",
    "        except (ConnectionRefusedError, ServiceUnavailable, TransientError, ConnectionAbortedError) as e:\n",
    "            print(f\"{e}. Try again...\")\n",
    "            sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_if_importing_is_successful()\n",
    "if not yelp_graph_ready:\n",
    "    raise Exception(\"Tried waiting for Neo4j Database Service, but still not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
