{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Preparing</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary libraries.\n",
    "import os, subprocess, uuid\n",
    "import regex as re\n",
    "import datetime\n",
    "from time import sleep\n",
    "\n",
    "import numpy as np, pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import json, csv\n",
    "from typing import List, Tuple, Set\n",
    "from pathlib import Path\n",
    "\n",
    "from py2neo import Graph\n",
    "from neo4j.exceptions import ServiceUnavailable, TransientError\n",
    "\n",
    "import shutil\n",
    "import reverse_geocoder as rg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Const.\n",
    "IN_CATEGORY = \"IN_CATEGORY\"\n",
    "IN_CITY = \"IN_CITY\"\n",
    "IN_AREA = \"IN_STATE\"\n",
    "IN_COUNTRY = \"IN_COUNTRY\"\n",
    "HAS = \"HAS\"\n",
    "FRIENDS = \"FRIENDS\"\n",
    "REVIEWS = \"REVIEWS_FOR\"\n",
    "WROTE = \"WROTE\"\n",
    "BUSINESS_NODE = \"Restaurant\"\n",
    "BUSINESS_ATTRS_NODE = \"B_Attribute\"\n",
    "USER_NODE = \"User\"\n",
    "REVIEW_NODE = \"Review\"\n",
    "CATEGORY_NODE = \"Category\"\n",
    "MENU_NODE = \"Menu\"\n",
    "PHOTO_NODE = \"Photo\"\n",
    "CITY_NODE = \"City\"\n",
    "AREA_NODE = \"State\"\n",
    "COUNTRY_NODE = \"Country\"\n",
    "\n",
    "CATEGORY_FILTER = [\"Restaurants\", \"Food\", \"Fast Food\", \"Sandwiches\", \"Breakfast & Brunch\", \n",
    "                   \"Coffee & Tea\", \"American\", \"Burgers\", \"Mexican\", \"Desserts\", \n",
    "                   \"Specialty Food\", \"Pizza\", \"Salad\", \"Italian\", \"Chicken Wings\",\n",
    "                  \"Tacos\", \"Seafood\", \"Vegetarian\", \"Ethnic Food\", \"Chinese\", \n",
    "                   \"Barbeque\", \"Mediterranean\", \"Japanese\", \"Korean\", \"Indian\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important locations.\n",
    "data_folder = r\"../../Dataset/yelp\"\n",
    "business_json_file = data_folder + \"/json/yelp_academic_dataset_business.json\"\n",
    "review_json_file = data_folder + \"/json/yelp_academic_dataset_review.json\"\n",
    "user_json_file = data_folder + \"/json/yelp_academic_dataset_user.json\"\n",
    "checkin_json_file = data_folder + \"/json/yelp_academic_dataset_checkin.json\"\n",
    "business_attrs_json_file = data_folder + \"/json/bussiness_attrs.json\"\n",
    "photos_json_file = data_folder + \"/photos/photos.json\"\n",
    "fixed_business_json_file = data_folder + \"/json/fixed_yelp_academic_dataset_business.json\"\n",
    "fixed_review_json_file = data_folder + \"/json/fixed_yelp_academic_dataset_review.json\"\n",
    "fixed_user_json_file = data_folder + \"/json/fixed_yelp_academic_dataset_user.json\"\n",
    "fixed_checkin_json_file = data_folder + \"/json/fixed_yelp_academic_dataset_checkin.json\"\n",
    "fixed_photos_json_file = data_folder + \"/json/fixed_photos.json\"\n",
    "fixed_business_attrs_json_file = data_folder +  \"/json/fixed_bussiness_attrs.json\"\n",
    "list_raw_files = [business_json_file, review_json_file, user_json_file, checkin_json_file, \n",
    "                  business_attrs_json_file, photos_json_file]\n",
    "list_fixed_data = [fixed_business_json_file, fixed_review_json_file, fixed_user_json_file, \n",
    "                   fixed_checkin_json_file, fixed_photos_json_file, fixed_business_attrs_json_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Csv files for Import Tool.\n",
    "# business_nodes_csv_file = data_folder + \"/neo4j/csv/business_nodes.csv\"\n",
    "# category_nodes_csv_file = data_folder + \"/neo4j/csv/category_nodes.csv\"\n",
    "# city_nodes_csv_file = data_folder + \"/neo4j/csv/city_nodes.csv\"\n",
    "# area_nodes_csv_file = data_folder + \"/neo4j/csv/area_nodes.csv\"\n",
    "# country_nodes_csv_file = data_folder + \"/neo4j/csv/country_nodes.csv\"\n",
    "# user_nodes_csv_file = data_folder + \"/neo4j/csv/user_nodes.csv\"\n",
    "# review_nodes_csv_file = data_folder + \"/neo4j/csv/review_nodes.csv\"\n",
    "# relationship_csv_file = data_folder + \"/neo4j/csv/relationships.csv\"\n",
    "# nodes_files = [business_nodes_csv_file, category_nodes_csv_file, city_nodes_csv_file, \n",
    "#                area_nodes_csv_file, country_nodes_csv_file, user_nodes_csv_file, review_nodes_csv_file]\n",
    "\n",
    "# business_nodes_csv_file = data_folder + \"/neo4j/csv/restaurant_nodes.csv\"\n",
    "# business_attrs_nodes_csv_file = data_folder + \"/neo4j/csv/business_attr_node.csv\"\n",
    "# category_nodes_csv_file = data_folder + \"/neo4j/csv/category_nodes.csv\"\n",
    "# photo_nodes_csv_file = data_folder + \"/neo4j/csv/photo_nodes.csv\"\n",
    "# city_nodes_csv_file = data_folder + \"/neo4j/csv/city_nodes.csv\"\n",
    "# area_nodes_csv_file = data_folder + \"/neo4j/csv/state_nodes.csv\"\n",
    "# country_nodes_csv_file = data_folder + \"/neo4j/csv/country_nodes.csv\"\n",
    "# user_nodes_csv_file = data_folder + \"/neo4j/csv/user_nodes.csv\"\n",
    "# review_nodes_csv_file = data_folder + \"/neo4j/csv/review_nodes.csv\"\n",
    "# relationship_csv_file = data_folder + \"/neo4j/csv/relationships.csv\"\n",
    "# relationship_business_attrs_csv_file = data_folder + \"/neo4j/csv/relationships_business_attrs.csv\"\n",
    "# nodes_files = [business_nodes_csv_file, business_attrs_nodes_csv_file, category_nodes_csv_file, \n",
    "#                photo_nodes_csv_file, city_nodes_csv_file, area_nodes_csv_file, country_nodes_csv_file, \n",
    "#                user_nodes_csv_file,  review_nodes_csv_file]\n",
    "\n",
    "restaurant_node_csv_file = \"/home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/restaurants.csv\"\n",
    "category_node_csv_file = \"/home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/category.csv\"\n",
    "city_node_csv_file = \"/home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/city.csv\"\n",
    "menu_node_csv_file = \"/home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/menu.csv\"\n",
    "state_node_csv_file = \"/home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/state.csv\"\n",
    "country_node_csv_file = \"/home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/country.csv\"\n",
    "user_node_csv_file = \"/home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/user.csv\"\n",
    "source_node_csv_file = \"/home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/source.csv\"\n",
    "review_node_csv_file = \"/home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/review.csv\"\n",
    "restaurant_attr_node_csv_file = \"/home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/restaurant_attr.csv\"\n",
    "relationship_csv_file = \"/home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/relation.csv\"\n",
    "nodes_files = [restaurant_node_csv_file, category_node_csv_file, city_node_csv_file, menu_node_csv_file,\n",
    "               state_node_csv_file, country_node_csv_file, user_node_csv_file, source_node_csv_file,\n",
    "               review_node_csv_file, restaurant_attr_node_csv_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection to Neo4j Db.\n",
    "graph_name = \"neo4j\" # Default for 4.0: neo4j; default for 3.5: graph.db\n",
    "SERVER_ADDRESS = \"bolt://localhost:7687\"\n",
    "SERVER_AUTH = (\"neo4j\", \"erclab\")\n",
    "graph = Graph(SERVER_ADDRESS, auth=SERVER_AUTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_home = graph.service.config[\"dbms.directories.neo4j_home\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/muzamil/.config/Neo4j Desktop/Application/neo4jDatabases/database-78a2aebd-c68b-431c-bc51-61237ddf706a/installation-4.1.0\n"
     ]
    }
   ],
   "source": [
    "print(neo4j_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# Check neo4j connection\n",
    "try:\n",
    "    graph.run(\"Match () Return 1 Limit 1\")\n",
    "    print('ok')\n",
    "except Exception:\n",
    "    print('not ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_files(files: List[str]):\n",
    "    for one_file in files:\n",
    "        one_path = Path(one_file)\n",
    "        if one_path.is_file():\n",
    "            one_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not all(list(map(lambda x: Path(x).is_file(), list_raw_files))):\n",
    "#     raise Exception(f'Not all Yelp raw files are available. Need: {list_raw_files}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Preprocessing Data</h1>\n",
    "\n",
    "<b>Data Problems:</b>\n",
    "\n",
    "    1. Ids are unique only in the specific domain. E.g. business_id in Business oiAlXZPIFm2nBCt0DHLu_Q is identical to that user_id in User. When using Neo4j's Import Tool, the relationship csv defines \"from-node\" and \"to-node\", identified by unique ids. Thus, the entities' ids, regardless of Business or User or Review must be unique.\n",
    "    2. Users have friends, but not all the friends exist. E.g. Unknown friend: oeMvJh94PiGQnx_6GlndPQ for User ntlvfPzc8eglqvk92iDIAw. Those unknown friends must be removed.\n",
    "    3. Some Business contain duplicate categories, e.g. Business with id HEGy1__jKyMhkhXRW3O1ZQ has duplicated Gas Stations. These must be deduplicated.\n",
    "    4. In Users, friends is supposed to be an array with string elements. But in reality, it is a string in which friend_ids are concatenated with commas. Same problem with categories field in Business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix data problems.\n",
    "def remove_unknown_friends(raw_path: str, output_path: str) -> None:\n",
    "    user_ids = set()\n",
    "    with open(raw_path, \"r\", encoding=\"utf-8\") as rf:\n",
    "        for line in rf:\n",
    "            if len(line.strip()) > 0:\n",
    "                json_node = json.loads(line)\n",
    "                user_ids.add(json_node[\"user_id\"])\n",
    "            \n",
    "    with open(raw_path, \"r\", encoding=\"utf-8\") as rf, open(output_path, mode=\"w\", encoding=\"utf-8\") as of:\n",
    "        for line in rf:\n",
    "            if len(line.strip()) > 0:\n",
    "                json_node = json.loads(line)\n",
    "                friends_str = json_node[\"friends\"]\n",
    "                if friends_str and len(friends_str.strip()) > 0:\n",
    "                    friends_arr = re.split(r\"\\s*,\\s*\", friends_str.strip())\n",
    "                    friends_exist_arr = list(filter(lambda x: x in user_ids, friends_arr))\n",
    "                    json_node[\"friends\"] = ', '.join(friends_exist_arr)\n",
    "                    of.write(f'{json.dumps(json_node)}\\n')\n",
    "\n",
    "                    \n",
    "def make_ids_unique(input_path: str, output_path: str, func_to_modify) -> None:\n",
    "    temp_output_file = str(uuid.uuid4())\n",
    "    line_count = len(open(input_path).readlines())\n",
    "    with open(input_path, mode=\"r\", encoding=\"utf-8\") as in_f, open(temp_output_file, mode=\"w\", encoding=\"utf-8\") as ou_f:\n",
    "        for line in tqdm(in_f, total=line_count):\n",
    "            if len(line.strip())>0:\n",
    "                json_node = json.loads(line)\n",
    "                json_node = func_to_modify(json_node)\n",
    "                if json_node is not None :\n",
    "                    ou_f.write(f'{json.dumps(json_node)}\\n')\n",
    "    # Rename the file.\n",
    "    os.replace(temp_output_file, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80455/80455 [00:00<00:00, 89437.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80455 b-pQeaRpvuhoEqudo3uymHIQ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def get_food_businesses():\n",
    "    business_ids = []\n",
    "    line_count = len(open(fixed_business_json_file).readlines())\n",
    "    with open(fixed_business_json_file) as ref:\n",
    "        for line in tqdm(ref, total=line_count):\n",
    "            blob = json.loads(line)\n",
    "            business_ids += [blob[\"business_id\"]]\n",
    "        print(str(len(business_ids))+\" \"+business_ids[0])\n",
    "    return business_ids\n",
    "\n",
    "business_ids = get_food_businesses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_foodie_reviewer():\n",
    "    user_ids = []\n",
    "    line_count = len(open(fixed_review_json_file).readlines())\n",
    "    with open(fixed_review_json_file) as ref:\n",
    "        for line in tqdm(ref, total=line_count):\n",
    "            blob = json.loads(line)\n",
    "            user_ids += [blob[\"user_id\"]]\n",
    "        print(str(len(user_ids))+\" \"+user_ids[0])\n",
    "    return user_ids\n",
    "\n",
    "user_ids = get_foodie_reviewer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(user_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_user(json_node):\n",
    "        json_node[\"user_id\"] = \"u-\" + json_node[\"user_id\"]\n",
    "        friends_str = json_node[\"friends\"]\n",
    "        if friends_str and len(friends_str.strip()) > 0:\n",
    "            friends_arr = re.split(r\"\\s*,\\s*\", friends_str.strip())\n",
    "            friends_arr = list(map(lambda x: \"u-\" + x, friends_arr))\n",
    "            json_node[\"friends\"] = ', '.join(friends_arr)\n",
    "        return json_node\n",
    "#         if df[0].eq(json_node[\"user_id\"]).any():\n",
    "#             return json_node\n",
    "#         else :\n",
    "#             return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1968703/1968703 [00:58<00:00, 33936.85it/s]\n"
     ]
    }
   ],
   "source": [
    "make_ids_unique(fixed_user_json_file, fixed_user_json_file, fix_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs_keys = [\"RestaurantsDelivery\", \"RestaurantsGoodForGroups\", \"RestaurantsTakeOut\", \"WiFi\", \n",
    "              \"BusinessAcceptsCreditCards\", \"BusinessParking\", \"BikeParking\", \"GoodForKids\"]\n",
    "def fix_business_attrs(input_path: str, output_path: str):\n",
    "    temp_output_file = str(uuid.uuid4())\n",
    "    with open(input_path) as json_file, open(temp_output_file, mode=\"w\", encoding=\"utf-8\") as ou_f:\n",
    "        data = json.load(json_file)\n",
    "        for business in data:\n",
    "            if \"attributes\" in business:\n",
    "                for key, value in list(business[\"attributes\"].items()):\n",
    "                    if key not in attrs_keys:\n",
    "                        del business[\"attributes\"][key]\n",
    "                        continue\n",
    "                        \n",
    "                    if \"RestaurantsDelivery\" == key:\n",
    "                        if value != \"True\":\n",
    "                            del business[\"attributes\"][key]\n",
    "                    if \"RestaurantsGoodForGroups\" == key:\n",
    "                        if value != \"True\":\n",
    "                            del business[\"attributes\"][key]\n",
    "                    if \"RestaurantsTakeOut\" == key:\n",
    "                        if value != \"True\":\n",
    "                            del business[\"attributes\"][key]\n",
    "                    if \"WiFi\" == key:\n",
    "                        if value != \"free\":\n",
    "                            del business[\"attributes\"][key]\n",
    "                    if \"BusinessAcceptsCreditCards\" == key:\n",
    "                        if value != \"True\":\n",
    "                            del business[\"attributes\"][key]\n",
    "                    if \"BusinessParking\" == key:\n",
    "                        if value == \"None\":\n",
    "                            del business[\"attributes\"][key]\n",
    "                        else :\n",
    "                            business[\"attributes\"][key] = \"True\"\n",
    "                    if \"BikeParking\" == key:\n",
    "                        if value != \"True\":\n",
    "                            del business[\"attributes\"][key]\n",
    "                    if \"GoodForKids\" == key:\n",
    "                        if value != \"True\":\n",
    "                            del business[\"attributes\"][key]\n",
    "\n",
    "                ou_f.write(f'{json.dumps(business)}\\n')\n",
    "    # Rename the file.\n",
    "    os.replace(temp_output_file, output_path)\n",
    "    \n",
    "fix_business_attrs(business_attrs_json_file, fixed_business_attrs_json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_users_with_review(input_path: str, output_path: str) -> None:\n",
    "    temp_output_file = str(uuid.uuid4())\n",
    "    line_count = len(open(input_path).readlines())\n",
    "    with open(input_path, mode=\"r\", encoding=\"utf-8\") as in_f, open(temp_output_file, mode=\"w\", encoding=\"utf-8\") as ou_f:\n",
    "        for line in tqdm(in_f, total=line_count):\n",
    "            if len(line.strip())>0:\n",
    "                json_node = json.loads(line)\n",
    "                json_df = pd.DataFrame([json_node])\n",
    "                if df[0].eq(json_df[\"user_id\"]).any():\n",
    "                    ou_f.write(f'{json.dumps(json_node)}\\n')\n",
    "    # Rename the file.\n",
    "    os.replace(temp_output_file, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_users_with_review(fixed_user_json_file, fixed_user_json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the fixed files are already available.\n",
    "if not all(list(map(lambda x: Path(x).is_file(), list_fixed_data))):\n",
    "    # Remove everything first.\n",
    "    delete_files(list_fixed_data)\n",
    "    # Re-create the fixed files.\n",
    "    def fix_user(json_node):\n",
    "        json_node[\"user_id\"] = \"u-\" + json_node[\"user_id\"]\n",
    "        friends_str = json_node[\"friends\"]\n",
    "        if friends_str and len(friends_str.strip()) > 0:\n",
    "            friends_arr = re.split(r\"\\s*,\\s*\", friends_str.strip())\n",
    "            friends_arr = list(map(lambda x: \"u-\" + x, friends_arr))\n",
    "            json_node[\"friends\"] = ', '.join(friends_arr)\n",
    "        return json_node\n",
    "        \n",
    "    def fix_business(json_node):\n",
    "        json_node[\"business_id\"] = \"b-\" + json_node[\"business_id\"]\n",
    "        if json_node[\"categories\"] and len(json_node[\"categories\"].strip())>0:\n",
    "            categories_arr = re.split(\"\\s*,\\s*\", json_node[\"categories\"].strip())\n",
    "            categories_set = set(categories_arr)\n",
    "            category_food_filter = list(filter(lambda x: x in CATEGORY_FILTER, categories_set))\n",
    "            if len(category_food_filter) > 0 :\n",
    "                json_node[\"categories\"] = ', '.join(categories_set)\n",
    "                return json_node\n",
    "            else :\n",
    "                return None\n",
    "            \n",
    "    def fix_review(json_node):\n",
    "        json_node[\"review_id\"] = \"r-\" + json_node[\"review_id\"]\n",
    "        json_node[\"user_id\"] = \"u-\" + json_node[\"user_id\"]\n",
    "        json_node[\"business_id\"] = \"b-\" + json_node[\"business_id\"]\n",
    "        if any(json_node[\"business_id\"] in s for s in business_ids):\n",
    "            return json_node\n",
    "        else :\n",
    "            return None\n",
    "        \n",
    "    def fix_checkin(json_node):\n",
    "        json_node[\"business_id\"] = \"b-\" + json_node[\"business_id\"]\n",
    "        if any(json_node[\"business_id\"] in s for s in business_ids):\n",
    "            return json_node\n",
    "        else :\n",
    "            return None\n",
    "        \n",
    "    def fix_photos(json_node):\n",
    "        json_node[\"photo_id\"] = \"p-\" + json_node[\"photo_id\"]\n",
    "        json_node[\"business_id\"] = \"b-\" + json_node[\"business_id\"]\n",
    "        if any(json_node[\"business_id\"] in s for s in business_ids):\n",
    "            return json_node\n",
    "        else :\n",
    "            return None\n",
    "\n",
    "    # Start fixing data problems.\n",
    "#     remove_unknown_friends(user_json_file, fixed_user_json_file)\n",
    "#     make_ids_unique(fixed_user_json_file, fixed_user_json_file, fix_user)\n",
    "#     make_ids_unique(business_json_file, fixed_business_json_file, fix_business)\n",
    "#     make_ids_unique(review_json_file, fixed_review_json_file, fix_review)\n",
    "#     make_ids_unique(checkin_json_file, fixed_checkin_json_file, fix_checkin)\n",
    "#     make_ids_unique(photos_json_file, fixed_photos_json_file, fix_photos)\n",
    "    make_ids_unique(photos_json_file, fixed_photos_json_file, fix_photos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [12:36<00:00, 264.43it/s]\n"
     ]
    }
   ],
   "source": [
    "def fix_photos(json_node):\n",
    "        json_node[\"photo_id\"] = \"p-\" + json_node[\"photo_id\"]\n",
    "        json_node[\"business_id\"] = \"b-\" + json_node[\"business_id\"]\n",
    "        if any(json_node[\"business_id\"] in s for s in business_ids):\n",
    "            return json_node\n",
    "        else :\n",
    "            return None\n",
    "\n",
    "make_ids_unique(photos_json_file, fixed_photos_json_file, fix_photos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Importing Data</h1>\n",
    "\n",
    "<h3>Generating inputs for Import Tool</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing business attrs relation for neo4j\n",
    "\n",
    "# Relationship writer.\n",
    "relationship_business_attrs_csv = open(relationship_business_attrs_csv_file, mode=\"w\", \n",
    "                                       encoding=\"utf-8\", newline=\"\\n\")\n",
    "relationship_business_attrs_fieldnames = [\":START_ID\", \":END_ID\", \":TYPE\"]\n",
    "relationship_business_attrs_writer = csv.DictWriter(relationship_business_attrs_csv, \n",
    "                                                    fieldnames=relationship_business_attrs_fieldnames, \n",
    "                                     delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "relationship_business_attrs_writer.writeheader()\n",
    "\n",
    "def write_business_attr_nodes_to_file():\n",
    "    with open(fixed_business_attrs_json_file, \"r\", encoding=\"utf-8\") as bajf:\n",
    "\n",
    "        # File too large, read line by line.\n",
    "        for line in bajf:\n",
    "            line = line.strip()\n",
    "            if len(line)>0:\n",
    "                json_node = json.loads(line)\n",
    "\n",
    "                for key, value in list(json_node[\"attributes\"].items()):\n",
    "                    relationship_business_attrs_writer.writerow({k:v for k, v in \n",
    "                                                                 zip(relationship_business_attrs_fieldnames, \n",
    "                                                                          [json_node[\"business_id\"], \n",
    "                                                                           key, HAS])})\n",
    "                \n",
    "    \n",
    "write_business_attr_nodes_to_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not all(list(map(lambda x: Path(x).is_file(), nodes_files))) or not Path(relationship_csv_file).is_file():\n",
    "    # Delete everything.\n",
    "    delete_files(nodes_files)\n",
    "    delete_files([relationship_csv_file])\n",
    "    # Using sets to ensures uniqueness.\n",
    "    business_lat_lon = {}\n",
    "    area_nodes: Set[Tuple[str, str]] = set()\n",
    "    city_nodes: Set[Tuple[str, str]]= set()\n",
    "    country_nodes: Set[str] = set()\n",
    "    categories_nodes: Set[str] = set()\n",
    "    \n",
    "    in_area_relationships: Set[Tuple[str, str, str]] = set()\n",
    "    in_country_relationships: Set[Tuple[str, str, str]] = set()\n",
    "    # Relationship writer.\n",
    "    relationship_csv = open(relationship_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\\n\")\n",
    "    relationship_fieldnames = [\":START_ID\", \":END_ID\", \":TYPE\"]\n",
    "    relationship_writer = csv.DictWriter(relationship_csv, fieldnames=relationship_fieldnames, \n",
    "                                         delimiter=\",\", quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "    relationship_writer.writeheader()\n",
    "    \n",
    "    # Func to write Business Nodes to file.\n",
    "    def write_business_nodes_to_file():\n",
    "        with open(fixed_business_json_file, \"r\", encoding=\"utf-8\") as bjf, open(\n",
    "            business_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\\n\") as business_nodes_csv:\n",
    "            fieldnames = [\"business_id:ID\", \"name\", \"address\", \"postal_code\", \"latitude\", \"longitude\", \n",
    "                          \"stars\", \"review_count\", \":LABEL\"]\n",
    "            writer = csv.DictWriter(business_nodes_csv, fieldnames=fieldnames, delimiter=\",\", \n",
    "                                    quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writeheader()\n",
    "            # File too large, read line by line.\n",
    "            for line in bjf:\n",
    "                line = line.strip()\n",
    "                if len(line)>0:\n",
    "                    json_node = json.loads(line)\n",
    "\n",
    "                    writer.writerow({k:v for k, v in zip(fieldnames, [json_node[\"business_id\"], \n",
    "                                                                      json_node[\"name\"], json_node[\"address\"],\n",
    "                                                                      json_node[\"postal_code\"], \n",
    "                                                                      json_node[\"latitude\"], \n",
    "                                                                      json_node[\"longitude\"],\n",
    "                                                                      json_node[\"stars\"], \n",
    "                                                                      json_node[\"review_count\"],BUSINESS_NODE])})\n",
    "                    business_lat_lon[json_node[\"business_id\"]] = (json_node[\"latitude\"], json_node[\"longitude\"])\n",
    "                            \n",
    "                    if json_node[\"categories\"] and len(json_node[\"categories\"].strip())>0: \n",
    "                        # can be None (e.g. for Business Id: 2W1tLg8ybRUEKMPoAPHTsQ)\n",
    "                        cur_categories = list(filter(lambda x: len(x)>0, \n",
    "                                                     map(lambda x: x.strip(), \n",
    "                                                         re.split(\"\\s*,\\s*\", json_node[\"categories\"].strip()))))\n",
    "                        categories_nodes.update(cur_categories)\n",
    "                        for category in cur_categories:\n",
    "                            relationship_writer.writerow({k:v for k, v in zip(relationship_fieldnames, \n",
    "                                                                              [json_node[\"business_id\"], \n",
    "                                                                               category, IN_CATEGORY])})\n",
    "    \n",
    "    # Func to write Category Nodes to file.\n",
    "    def write_category_nodes_to_file():\n",
    "        with open(category_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\\n\") as category_nodes_csv:\n",
    "            fieldnames = [\"category_id:ID\", \":LABEL\"]\n",
    "            writer = csv.DictWriter(category_nodes_csv, fieldnames=fieldnames, delimiter=\",\", \n",
    "                                    quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writeheader()\n",
    "            for category_id in categories_nodes:\n",
    "                writer.writerow({k:v for k, v in zip(fieldnames, [category_id, CATEGORY_NODE])})\n",
    "    \n",
    "    def write_business_attr_nodes_to_file():\n",
    "        with open(fixed_business_attrs_json_file, \"r\", encoding=\"utf-8\") as bajf:\n",
    "\n",
    "            # File too large, read line by line.\n",
    "            for line in bajf:\n",
    "                line = line.strip()\n",
    "                if len(line)>0:\n",
    "                    json_node = json.loads(line)\n",
    "                    for key, value in list(json_node[\"attributes\"].items()):\n",
    "                        relationship_writer.writerow({k:v for k, v in zip(relationship_fieldnames, \n",
    "                                                                              [json_node[\"business_id\"], \n",
    "                                                                               key, HAS])})\n",
    "    # Func to make City, Area, Country Nodes.\n",
    "    def make_city_area_country_nodes():\n",
    "        lat_lons = list(business_lat_lon.values())\n",
    "        city_state_countries = rg.search(lat_lons)\n",
    "        # Process City, Area, Country Nodes; save IN_CITY relationship to file.\n",
    "        for (business_id, city_state_country) in list(zip(list(business_lat_lon.keys()), city_state_countries)):\n",
    "            city, state, country = city_state_country[\"name\"], city_state_country[\"admin1\"], city_state_country[\"cc\"]\n",
    "            unique_state = f'{state}-{country}'\n",
    "            unique_city = f'{city}-{state}-{country}'\n",
    "            country_nodes.add(country)\n",
    "            area_nodes.add((unique_state, state))\n",
    "            city_nodes.add((unique_city, city))\n",
    "            # Create corresponding relationships.\n",
    "            relationship_writer.writerow({k:v for k, v in zip(relationship_fieldnames, \n",
    "                                                              [business_id, unique_city, IN_CITY])})\n",
    "            in_area_relationships.add((unique_city, unique_state, IN_AREA))\n",
    "            in_country_relationships.add((unique_state, country, IN_COUNTRY))\n",
    "    \n",
    "    # Func to write City, Area, Country Nodes to file.\n",
    "    def write_city_area_country_nodes_to_file():\n",
    "        with open(city_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\\n\") as city_nodes_csv:\n",
    "            fieldnames = [\"city_id:ID\", \"name\", \":LABEL\"]\n",
    "            writer = csv.DictWriter(city_nodes_csv, fieldnames=fieldnames, delimiter=\",\", \n",
    "                                    quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writeheader()\n",
    "            for (city_id, city_name) in city_nodes:\n",
    "                writer.writerow({k:v for k, v in zip(fieldnames, [city_id, city_name, CITY_NODE])})\n",
    "\n",
    "        with open(area_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\\n\") as area_nodes_csv:\n",
    "            fieldnames = [\"area_id:ID\", \"name\", \":LABEL\"]\n",
    "            writer = csv.DictWriter(area_nodes_csv, fieldnames=fieldnames, delimiter=\",\", \n",
    "                                    quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writeheader()\n",
    "            for (area_id, area_name) in area_nodes:\n",
    "                writer.writerow({k:v for k, v in zip(fieldnames, [area_id, area_name, AREA_NODE])})\n",
    "\n",
    "        with open(country_nodes_csv_file, mode=\"w\", encoding=\"utf-8\", newline=\"\\n\") as country_nodes_csv:\n",
    "            fieldnames = [\"country_id:ID\", \":LABEL\"]\n",
    "            writer = csv.DictWriter(country_nodes_csv, fieldnames=fieldnames, delimiter=\",\", \n",
    "                                    quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writeheader()\n",
    "            for country_id in country_nodes:\n",
    "                writer.writerow({k:v for k, v in zip(fieldnames, [country_id, COUNTRY_NODE])})\n",
    "                \n",
    "        for (u1, u2, rel_type) in in_area_relationships:\n",
    "            relationship_writer.writerow({k:v for k, v in zip(relationship_fieldnames, [u1, u2, rel_type])})\n",
    "\n",
    "        for (u1, u2, rel_type) in in_country_relationships:\n",
    "            relationship_writer.writerow({k:v for k, v in zip(relationship_fieldnames, [u1, u2, rel_type])})\n",
    "    \n",
    "    # Func to write Review nodes to file.\n",
    "    def write_photo_nodes_to_file():\n",
    "        with open(fixed_photos_json_file, \"r\", encoding=\"utf-8\") as rjf, open(photo_nodes_csv_file, \n",
    "                                                                              mode=\"w\", encoding=\"utf-8\", \n",
    "                                                                              newline=\"\\n\") as checkin_nodes_csv:\n",
    "            fieldnames = [\"photo_id:ID\", \"caption\", \"label\", \":LABEL\"]\n",
    "            writer = csv.DictWriter(checkin_nodes_csv, fieldnames=fieldnames, delimiter=\",\", \n",
    "                                    quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writeheader()\n",
    "            # File too large, read line by line.\n",
    "            for line in rjf:\n",
    "                line = line.strip()\n",
    "                if len(line)>0:\n",
    "                    json_node = json.loads(line)\n",
    "                    writer.writerow({k:v for k, v in zip(fieldnames, [json_node[\"photo_id\"], json_node[\"caption\"],\n",
    "                                                                      json_node[\"label\"], PHOTO_NODE])})\n",
    "                    relationship_writer.writerow({k:v for k, v in zip(relationship_fieldnames, \n",
    "                                                                      [json_node[\"business_id\"], \n",
    "                                                                       json_node[\"photo_id\"], HAS])})\n",
    "                    \n",
    "    # Func to write User nodes to file.\n",
    "    def write_user_nodes_to_file():\n",
    "        friend_relationships: Set[Tuple[str, str, str]] = set()\n",
    "        with open(fixed_user_json_file, \"r\", encoding=\"utf-8\") as ujf, open(user_nodes_csv_file, \n",
    "                                                                            mode=\"w\", encoding=\"utf-8\", \n",
    "                                                                            newline=\"\\n\") as user_nodes_csv:\n",
    "            fieldnames = [\"user_id:ID\", \"name\", \"review_count\", \"yelping_since\", \"average_star\", \":LABEL\"]\n",
    "            writer = csv.DictWriter(user_nodes_csv, fieldnames=fieldnames, delimiter=\",\", quotechar='\"', \n",
    "                                    quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writeheader()\n",
    "            # File too large, read line by line.\n",
    "            for line in ujf:\n",
    "                line = line.strip()\n",
    "                if len(line)>0:\n",
    "                    json_node = json.loads(line)\n",
    "                    writer.writerow({k:v for k, v in zip(fieldnames, [json_node[\"user_id\"], \n",
    "                                                                      json_node[\"name\"], \n",
    "                                                                      json_node[\"review_count\"],\n",
    "                                                                      json_node[\"yelping_since\"], \n",
    "                                                                      json_node[\"average_stars\"], \n",
    "                                                                      USER_NODE])})\n",
    "                    if json_node[\"friends\"] and len(json_node[\"friends\"].strip()) > 0:\n",
    "                        friends_arr = re.split(\"\\s*,\\s*\", json_node[\"friends\"].strip())\n",
    "                        for friend_id in friends_arr:\n",
    "                            # Prevent duplicate friend relationship later!\n",
    "                            f1 = min(json_node[\"user_id\"], friend_id)\n",
    "                            f2 = max(json_node[\"user_id\"], friend_id)\n",
    "                            friend_relationships.add((f1, f2, FRIENDS))\n",
    "        \n",
    "        for (f1, f2, rel_type) in friend_relationships:\n",
    "            relationship_writer.writerow({k:v for k, v in zip(relationship_fieldnames, [f1, f2, rel_type])})\n",
    "    \n",
    "    # Func to write Review nodes to file.\n",
    "    def write_review_nodes_to_file():\n",
    "        with open(fixed_review_json_file, \"r\", encoding=\"utf-8\") as rjf, open(review_nodes_csv_file, \n",
    "                                                                              mode=\"w\", encoding=\"utf-8\", \n",
    "                                                                              newline=\"\\n\") as review_nodes_csv:\n",
    "            fieldnames = [\"review_id:ID\", \"stars\", \"date\", \"text\", \":LABEL\"]\n",
    "            writer = csv.DictWriter(review_nodes_csv, fieldnames=fieldnames, delimiter=\",\", \n",
    "                                    quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "            writer.writeheader()\n",
    "            # File too large, read line by line.\n",
    "            for line in rjf:\n",
    "                line = line.strip()\n",
    "                if len(line)>0:\n",
    "                    json_node = json.loads(line)\n",
    "                    writer.writerow({k:v for k, v in zip(fieldnames, [json_node[\"review_id\"], \n",
    "                                                                      json_node[\"stars\"], json_node[\"date\"], \n",
    "                                                                      json_node[\"text\"], REVIEW_NODE])})\n",
    "                    relationship_writer.writerow({k:v for k, v in zip(relationship_fieldnames, \n",
    "                                                                      [json_node[\"user_id\"], \n",
    "                                                                       json_node[\"review_id\"], WROTE])})\n",
    "                    relationship_writer.writerow({k:v for k, v in zip(relationship_fieldnames, \n",
    "                                                                      [json_node[\"review_id\"], \n",
    "                                                                       json_node[\"business_id\"], REVIEWS])})\n",
    "    \n",
    "    # Start re-creating CSV files for Import Tool.\n",
    "    write_business_nodes_to_file()\n",
    "    write_category_nodes_to_file()\n",
    "    write_business_attr_nodes_to_file()\n",
    "    del categories_nodes\n",
    "    make_city_area_country_nodes()\n",
    "    write_city_area_country_nodes_to_file()\n",
    "    write_photo_nodes_to_file()\n",
    "    del business_lat_lon, city_nodes, area_nodes, country_nodes, in_area_relationships, in_country_relationships\n",
    "    write_user_nodes_to_file()\n",
    "    write_review_nodes_to_file()\n",
    "    \n",
    "    relationship_csv.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Checking Data Integrity</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_nodes = 0\n",
    "\n",
    "# Func to if nodes are unique and relationships are also unique.\n",
    "def check_nodes_relationships_csv_files_integrity():\n",
    "    global num_of_nodes\n",
    "    # Nodes files.\n",
    "    for one_node_file in nodes_files:\n",
    "        print(\"one_node_file: \"+one_node_file)\n",
    "        temp_df = pd.read_csv(one_node_file, header = \"infer\", sep = \",\", encoding = \"utf-8\")\n",
    "        if len(temp_df.iloc[:, 0]) != len(np.unique(temp_df.iloc[:, 0].values)):\n",
    "            raise Exception(f'Nodes in [{one_node_file}]: not unique')\n",
    "        num_of_nodes += len(temp_df.iloc[:, 0])\n",
    "    # Relationship file.\n",
    "    temp_df = pd.read_csv(relationship_csv_file, header = \"infer\", sep = \",\", encoding = \"utf-8\")\n",
    "    rels = temp_df.iloc[:, 0] + temp_df.iloc[:, 1]\n",
    "    if len(rels) != len(np.unique(rels.values)):\n",
    "        raise Exception(f'Relationships in [{relationship_csv_file}]: not unique')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_node_file: /home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/restaurants.csv\n",
      "one_node_file: /home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/category.csv\n",
      "one_node_file: /home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/city.csv\n",
      "one_node_file: /home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/menu.csv\n",
      "one_node_file: /home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/state.csv\n",
      "one_node_file: /home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/country.csv\n",
      "one_node_file: /home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/user.csv\n",
      "one_node_file: /home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/source.csv\n",
      "one_node_file: /home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/review.csv\n",
      "one_node_file: /home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/restaurant_attr.csv\n"
     ]
    }
   ],
   "source": [
    "check_nodes_relationships_csv_files_integrity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importing to Neo4j</h3>\n",
    "\n",
    "<b>Reseting the database and importing data</b>\n",
    "\n",
    "The quickest/cleanest/safest way is:\n",
    "\n",
    "    1. Stop the Neo4j's Database Service.\n",
    "    2. Remove the neo4j's Database's Data folder.\n",
    "    3. Execute the Import Tool.\n",
    "    4. Start the Neo4j's Database Service.\n",
    "\n",
    "<b>Important Note:</b> Users must have privileges to start/stop Windows services. If needing elevating, a Windows popup will appear, asking for the permission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Func to control Neo4j's Database Service.\n",
    "def command_neo4j_database_service(cmd: str):\n",
    "    neo4j_cmd = neo4j_home + r\"/bin/neo4j\"\n",
    "    if cmd in [\"stop\", \"start\"]:\n",
    "        cmd_res = subprocess.run([neo4j_cmd, cmd], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        cmd_stdout = str(cmd_res.stdout, \"utf-8\")\n",
    "        print(cmd_stdout)\n",
    "#         if cmd == \"stop\" and \"stopped\" not in cmd_stdout:\n",
    "#             raise Exception(f\"Can't stop Neo4j's Database Service [{cmd_res.stderr}]\")\n",
    "#         elif cmd == \"start\" and \"started\" not in cmd_stdout:\n",
    "#             raise Exception(f\"Can't start Neo4j's Database Service [{cmd_res.stderr}]\")\n",
    "    else:\n",
    "        raise Exception(f'Unknown command for Neo4j\\'s Database Service: [{cmd}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Func to remove the Database's Data folder.\n",
    "def reset_neo4j_database():\n",
    "    neo4j_graph_folder = rf\"{neo4j_home}/data/databases/{graph_name}\"\n",
    "    neo4j_trans_folder = rf\"{neo4j_home}/data/transactions/{graph_name}\"\n",
    "    print(neo4j_graph_folder)\n",
    "    print(neo4j_trans_folder)\n",
    "    if not Path(neo4j_graph_folder).is_dir():\n",
    "        raise Exception(\"Can't find Neo4j's Database's Data folder\")\n",
    "    shutil.rmtree(neo4j_graph_folder)\n",
    "    shutil.rmtree(neo4j_trans_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Func to import data into Neo4j's Database.\n",
    "def import_data():\n",
    "    import_tool_cmd = neo4j_home + r\"/bin/neo4j-admin\"\n",
    "    arguments = [\"import\", \"--multiline-fields=true\"] + list(map(lambda x: f'--nodes={x}', nodes_files)) + [f\"--relationships={relationship_csv_file}\"]\n",
    "    print(arguments)\n",
    "    # Execute the Import Tool (This tool will re-create a fresh \"neo4j\" folder if needed).\n",
    "    cmd_res = subprocess.run([import_tool_cmd] + arguments, cwd=os.getcwd(), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "    print(str(cmd_res.stdout, \"utf-8\"))\n",
    "    if \"IMPORT DONE\" not in str(cmd_res.stdout, \"utf-8\"):\n",
    "        raise Exception(f\"Can't execute Import Tool successfully [{cmd_res.stderr}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['import', '--multiline-fields=true', '--nodes=/home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/restaurants.csv', '--nodes=/home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/category.csv', '--nodes=/home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/city.csv', '--nodes=/home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/menu.csv', '--nodes=/home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/state.csv', '--nodes=/home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/country.csv', '--nodes=/home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/user.csv', '--nodes=/home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/source.csv', '--nodes=/home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/review.csv', '--nodes=/home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/restaurant_attr.csv', '--relationships=/home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/relation.csv']\n",
      "WARNING: Max 4096 open files allowed, minimum of 40000 recommended. See the Neo4j manual.\n",
      "Neo4j version: 4.1.0\n",
      "Importing the contents of these files into /home/muzamil/.config/Neo4j Desktop/Application/neo4jDatabases/database-78a2aebd-c68b-431c-bc51-61237ddf706a/installation-4.1.0/data/databases/neo4j:\n",
      "Nodes:\n",
      "  /home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/restaurants.csv\n",
      "  /home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/category.csv\n",
      "  /home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/city.csv\n",
      "  /home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/menu.csv\n",
      "  /home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/state.csv\n",
      "  /home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/country.csv\n",
      "  /home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/user.csv\n",
      "  /home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/source.csv\n",
      "  /home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/review.csv\n",
      "  /home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/restaurant_attr.csv\n",
      "\n",
      "Relationships:\n",
      "  /home/muzamil/Projects/Python/ML/NLP/KG/Restaurant_KG/dataset/relation.csv\n",
      "\n",
      "\n",
      "Available resources:\n",
      "  Total machine memory: 15.49GiB\n",
      "  Free machine memory: 2.077GiB\n",
      "  Max heap memory : 1.778GiB\n",
      "  Processors: 6\n",
      "  Configured max memory: 12.34GiB\n",
      "  High-IO: false\n",
      "\n",
      "\n",
      "Import starting 2021-06-25 00:17:44.102+0900\n",
      "  Estimated number of nodes: 50.00 \n",
      "  Estimated number of node properties: 121.00 \n",
      "  Estimated number of relationships: 47.00 \n",
      "  Estimated number of relationship properties: 0.00 \n",
      "  Estimated disk space usage: 10.07KiB\n",
      "  Estimated required memory usage: 1020MiB\n",
      "\n",
      "(1/4) Node import 2021-06-25 00:17:45.211+0900\n",
      "  Estimated number of nodes: 50.00 \n",
      "  Estimated disk space usage: 8.506KiB\n",
      "  Estimated required memory usage: 1020MiB\n",
      "-......... .......... .......... .......... ..........   5% ∆139ms\n",
      ".......... .......... .......... .......... ..........  10% ∆1ms\n",
      ".......... .......... .......... .......... ..........  15% ∆1ms\n",
      ".......... .......... .......... .......... ..........  20% ∆0ms\n",
      ".......... .......... .......... .......... ..........  25% ∆0ms\n",
      ".......... .......... .......... .......... ..........  30% ∆0ms\n",
      ".......... .......... .......... .......... ..........  35% ∆1ms\n",
      ".......... .......... .......... .......... ..........  40% ∆0ms\n",
      ".......... .......... .......... .......... ..........  45% ∆0ms\n",
      ".......... .......... .......... .......... ..........  50% ∆1ms\n",
      ".......... .......... .......... .......... ..........  55% ∆0ms\n",
      ".......... .......... .......... .......... ..........  60% ∆0ms\n",
      ".......... .......... .......... .......... ..........  65% ∆0ms\n",
      ".......... .......... .......... .......... ..........  70% ∆1ms\n",
      ".......... .......... .......... .......... ..........  75% ∆0ms\n",
      ".......... .......... .......... .......... ..........  80% ∆0ms\n",
      ".......... .......... .......... .......... ..........  85% ∆0ms\n",
      ".......... .......... .......... .......... ..........  90% ∆0ms\n",
      ".......... .......... .......... .......... ..........  95% ∆1ms\n",
      ".......... .......... .......... .......... .......... 100% ∆0ms\n",
      "\n",
      "(2/4) Relationship import 2021-06-25 00:17:46.519+0900\n",
      "  Estimated number of relationships: 47.00 \n",
      "  Estimated disk space usage: 1.561KiB\n",
      "  Estimated required memory usage: 1.004GiB\n",
      ".......... .......... .......... .......... ..........   5% ∆810ms\n",
      ".......... .......... .......... .......... ..........  10% ∆0ms\n",
      ".......... .......... .......... .......... ..........  15% ∆0ms\n",
      ".......... .......... .......... .......... ..........  20% ∆0ms\n",
      ".......... .......... .......... .......... ..........  25% ∆0ms\n",
      ".......... .......... .......... .......... ..........  30% ∆0ms\n",
      ".......... .......... .......... .......... ..........  35% ∆0ms\n",
      ".......... .......... .......... .......... ..........  40% ∆1ms\n",
      ".......... .......... .......... .......... ..........  45% ∆0ms\n",
      ".......... .......... .......... .......... ..........  50% ∆0ms\n",
      ".......... .......... .......... .......... ..........  55% ∆0ms\n",
      ".......... .......... .......... .......... ..........  60% ∆0ms\n",
      ".......... .......... .......... .......... ..........  65% ∆0ms\n",
      ".......... .......... .......... .......... ..........  70% ∆0ms\n",
      ".......... .......... .......... .......... ..........  75% ∆0ms\n",
      ".......... .......... .......... .......... ..........  80% ∆0ms\n",
      ".......... .......... .......... .......... ..........  85% ∆1ms\n",
      ".......... .......... .......... .......... ..........  90% ∆0ms\n",
      ".......... .......... .......... .......... ..........  95% ∆0ms\n",
      ".......... .......... .......... .......... .......... 100% ∆0ms\n",
      "\n",
      "(3/4) Relationship linking 2021-06-25 00:17:47.331+0900\n",
      "  Estimated required memory usage: 1020MiB\n",
      "-......... .......... .......... .......... ..........   5% ∆553ms\n",
      ".......... .......... .......... .......... ..........  10% ∆0ms\n",
      ".......... .......... .......... .......... ..........  15% ∆0ms\n",
      ".......... .......... .......... .......... ..........  20% ∆0ms\n",
      ".......... .......... .......... .......... ..........  25% ∆0ms\n",
      ".......... .......... .......... .......... ..........  30% ∆0ms\n",
      ".......... .......... .......... .......... ..........  35% ∆1ms\n",
      ".......... .......... .......... .......... ..........  40% ∆0ms\n",
      ".......... .......... .......... .......... ..........  45% ∆0ms\n",
      ".......... .......... .......... .......... ..........  50% ∆0ms\n",
      ".......... .......... .......... .......... ..........  55% ∆0ms\n",
      ".......... .......... .......... .......... ..........  60% ∆0ms\n",
      ".......... .......... .......... .......... ..........  65% ∆0ms\n",
      ".......... .......... .......... .......... ..........  70% ∆0ms\n",
      ".......... .......... .......... .......... ..........  75% ∆0ms\n",
      ".......... .......... .......... .......... ..........  80% ∆0ms\n",
      ".......... .......... .......... .......... ..........  85% ∆0ms\n",
      ".......... .......... .......... .......... ..........  90% ∆1ms\n",
      ".......... .......... .......... .......... ..........  95% ∆0ms\n",
      ".......... .......... .......... .......... .......... 100% ∆0ms\n",
      "\n",
      "(4/4) Post processing 2021-06-25 00:17:48.000+0900\n",
      "  Estimated required memory usage: 1020MiB\n",
      "-......... .......... .......... .......... ..........   5% ∆763ms\n",
      ".......... .......... .......... .......... ..........  10% ∆0ms\n",
      ".......... .......... .......... .......... ..........  15% ∆1ms\n",
      ".......... .......... .......... .......... ..........  20% ∆0ms\n",
      ".......... .......... .......... .......... ..........  25% ∆0ms\n",
      ".......... .......... .......... .......... ..........  30% ∆1ms\n",
      ".......... .......... .......... .......... ..........  35% ∆0ms\n",
      ".......... .......... .......... .......... ..........  40% ∆1ms\n",
      ".......... .......... .......... .......... ..........  45% ∆0ms\n",
      ".......... .......... .......... .......... ..........  50% ∆0ms\n",
      ".......... .......... .......... .......... ..........  55% ∆0ms\n",
      ".......... .......... .......... .......... ..........  60% ∆0ms\n",
      ".......... .......... .......... .......... ..........  65% ∆0ms\n",
      ".......... .......... .......... .......... ..........  70% ∆1ms\n",
      ".......... .......... .......... .......... ..........  75% ∆0ms\n",
      ".......... .......... .......... .......... ..........  80% ∆0ms\n",
      ".......... .......... .......... .......... ..........  85% ∆0ms\n",
      ".......... .......... .......... .......... ..........  90% ∆0ms\n",
      ".......... .......... .......... .......... ..........  95% ∆0ms\n",
      ".......... .......... .......... .......... .......... 100% ∆0ms\n",
      "\n",
      "\n",
      "IMPORT DONE in 6s 707ms. \n",
      "Imported:\n",
      "  46 nodes\n",
      "  47 relationships\n",
      "  113 properties\n",
      "Peak memory usage: 1.004GiB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check if Neo4j's Database Service is already containing the desired data.\n",
    "if not(len(graph.nodes) == num_of_nodes and num_of_nodes > 0):\n",
    "    # Start resetting the database, then importing the data (nodes & relationships).\n",
    "#     command_neo4j_database_service(\"stop\")\n",
    "#     reset_neo4j_database()\n",
    "    import_data()\n",
    "#     command_neo4j_database_service(\"start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp_graph_ready = False\n",
    "\n",
    "# Func to check if the importing process was done successfully.\n",
    "def check_if_importing_is_successful():\n",
    "    global graph\n",
    "    global yelp_graph_ready\n",
    "    num_tries = 30\n",
    "    for one_try in range(num_tries):\n",
    "        try:\n",
    "            graph = Graph(SERVER_ADDRESS, auth=SERVER_AUTH)\n",
    "            cur_num_nodes = len(graph.nodes)\n",
    "            if cur_num_nodes == 0:\n",
    "                raise Exception(f\"There is no node in the Database\")\n",
    "            if cur_num_nodes != num_of_nodes:\n",
    "                raise Exception(f'Expected: [{num_of_nodes}] nodes, but found: [{cur_num_nodes}]')\n",
    "            yelp_graph_ready = True\n",
    "        except (ConnectionRefusedError, ServiceUnavailable, TransientError, ConnectionAbortedError) as e:\n",
    "            print(f\"{e}. Try again...\")\n",
    "            sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "There is no node in the Database",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-fad36d0072e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcheck_if_importing_is_successful\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0myelp_graph_ready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Tried waiting for Neo4j Database Service, but still not available\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-61be51141483>\u001b[0m in \u001b[0;36mcheck_if_importing_is_successful\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mcur_num_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcur_num_nodes\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"There is no node in the Database\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcur_num_nodes\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mnum_of_nodes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Expected: [{num_of_nodes}] nodes, but found: [{cur_num_nodes}]'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: There is no node in the Database"
     ]
    }
   ],
   "source": [
    "check_if_importing_is_successful()\n",
    "if not yelp_graph_ready:\n",
    "    raise Exception(\"Tried waiting for Neo4j Database Service, but still not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
